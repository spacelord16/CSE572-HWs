{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b670d19",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction - Machine Learning Project\n",
    "\n",
    "## Important: Running Order\n",
    "\n",
    "**Please run the cells in order from top to bottom.** If you encounter errors, it's likely because cells were run out of order. In that case, restart the kernel and run all cells sequentially.\n",
    "\n",
    "The notebook follows this workflow:\n",
    "\n",
    "1. Load and combine data\n",
    "2. Feature engineering and preprocessing\n",
    "3. Model training and evaluation\n",
    "4. Results visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a0a8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import classifiers from scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set plot style\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26255ff",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data\n",
    "\n",
    "First, we'll load the `train.csv` and `test.csv` files. For consistent preprocessing, we'll combine them into a single DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ca94a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Training data shape: (891, 12)\n",
      "Testing data shape: (418, 11)\n",
      "Combined data shape: (1309, 12)\n"
     ]
    }
   ],
   "source": [
    "# Load the training and testing data\n",
    "try:\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    print(\"Data loaded successfully!\")\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Testing data shape: {test_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Ensure 'train.csv' and 'test.csv' are in the same directory.\")\n",
    "\n",
    "# Combine datasets for easier preprocessing\n",
    "all_data = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\n",
    "print(f\"Combined data shape: {all_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c64bef",
   "metadata": {},
   "source": [
    "## Step 2: Improved Data Preprocessing & Feature Engineering\n",
    "\n",
    "Here we apply our enhanced preprocessing strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5a54b",
   "metadata": {},
   "source": [
    "### a. Feature Engineering from 'Cabin'\n",
    "\n",
    "We extract the first letter of the `Cabin` number to create a new `Deck` feature. Missing values are filled with 'U' for 'Unknown'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b70f376c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'Deck' feature. Unique values: ['U' 'C' 'E' 'G' 'D' 'A' 'B' 'F' 'T']\n"
     ]
    }
   ],
   "source": [
    "all_data['Deck'] = all_data['Cabin'].apply(lambda x: str(x)[0] if pd.notnull(x) else 'U')\n",
    "print(\"Created 'Deck' feature. Unique values:\", all_data['Deck'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed7381",
   "metadata": {},
   "source": [
    "### b. Create 'FamilySize' Feature\n",
    "\n",
    "We combine `SibSp` and `Parch` to get the total family size and then group it into categorical bins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "993df540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'FamilySize_cat' feature.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>FamilySize_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Alone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FamilySize FamilySize_cat\n",
       "0           2          Small\n",
       "1           2          Small\n",
       "2           1          Alone\n",
       "3           2          Small\n",
       "4           1          Alone"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['FamilySize'] = all_data['SibSp'] + all_data['Parch'] + 1\n",
    "all_data['FamilySize_cat'] = pd.cut(all_data['FamilySize'], bins=[0, 1, 4, 20], labels=['Alone', 'Small', 'Large'])\n",
    "print(\"Created 'FamilySize_cat' feature.\")\n",
    "all_data[['FamilySize', 'FamilySize_cat']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81281ac4",
   "metadata": {},
   "source": [
    "### c. Impute Missing 'Fare' and 'Embarked'\n",
    "\n",
    "We fill the single missing `Fare` value with the median and the two missing `Embarked` values with the mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9650ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 'Fare' and 'Embarked' imputed.\n"
     ]
    }
   ],
   "source": [
    "all_data['Fare'] = all_data['Fare'].fillna(all_data['Fare'].median())\n",
    "all_data['Embarked'] = all_data['Embarked'].fillna(all_data['Embarked'].mode()[0])\n",
    "print(\"Missing 'Fare' and 'Embarked' imputed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fe68a0",
   "metadata": {},
   "source": [
    "### d. Advanced Imputation for 'Age'\n",
    "\n",
    "Instead of using a single overall median, we fill missing `Age` values with the median age specific to each passenger's `Pclass` and `Sex`. This is a more accurate estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "619b14d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in all_data:\n",
      "['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Deck', 'FamilySize', 'FamilySize_cat']\n",
      "\n",
      "Checking if required columns exist:\n",
      "'Age' in columns: True\n",
      "'Pclass' in columns: True\n",
      "'Sex' in columns: True\n",
      "\n",
      "Median age by Pclass and Sex:\n",
      "                Age\n",
      "Pclass Sex         \n",
      "1      female  36.0\n",
      "       male    42.0\n",
      "2      female  28.0\n",
      "       male    29.5\n",
      "3      female  22.0\n",
      "       male    25.0\n",
      "\n",
      "Missing 'Age' values imputed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_b/dy79ryls7j58krx5q5vbwf200000gn/T/ipykernel_6009/3172604743.py:11: FutureWarning: The provided callable <function median at 0x105ff2520> is currently using DataFrameGroupBy.median. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"median\" instead.\n",
      "  age_impute_table = all_data.pivot_table(values='Age', index=['Pclass', 'Sex'], aggfunc=np.median)\n"
     ]
    }
   ],
   "source": [
    "# Check what columns are available before proceeding\n",
    "print(\"Available columns in all_data:\")\n",
    "print(all_data.columns.tolist())\n",
    "print(\"\\nChecking if required columns exist:\")\n",
    "print(\"'Age' in columns:\", 'Age' in all_data.columns)\n",
    "print(\"'Pclass' in columns:\", 'Pclass' in all_data.columns)\n",
    "print(\"'Sex' in columns:\", 'Sex' in all_data.columns)\n",
    "\n",
    "# Only proceed with age imputation if the required columns exist\n",
    "if all(col in all_data.columns for col in ['Age', 'Pclass', 'Sex']):\n",
    "    age_impute_table = all_data.pivot_table(values='Age', index=['Pclass', 'Sex'], aggfunc=np.median)\n",
    "    print(\"\\nMedian age by Pclass and Sex:\")\n",
    "    print(age_impute_table)\n",
    "\n",
    "    # Fix the age imputation to ensure we get scalar values\n",
    "    def impute_age(row):\n",
    "        if pd.isnull(row['Age']):\n",
    "            return age_impute_table.loc[(row['Pclass'], row['Sex']), 'Age']\n",
    "        else:\n",
    "            return row['Age']\n",
    "\n",
    "    all_data['Age'] = all_data.apply(impute_age, axis=1)\n",
    "    print(\"\\nMissing 'Age' values imputed.\")\n",
    "else:\n",
    "    print(\"\\nRequired columns not found. Age imputation skipped.\")\n",
    "    print(\"This may be because the notebook is being run out of order.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb7c2edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age column converted to numeric.\n",
      "Age column data type: float64\n",
      "Age column info:\n",
      "count    1309.000000\n",
      "mean       29.261398\n",
      "std        13.218275\n",
      "min         0.170000\n",
      "25%        22.000000\n",
      "50%        26.000000\n",
      "75%        36.000000\n",
      "max        80.000000\n",
      "Name: Age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Ensure Age column is numeric and handle any remaining issues\n",
    "if 'Age' in all_data.columns:\n",
    "    all_data['Age'] = pd.to_numeric(all_data['Age'], errors='coerce')\n",
    "    print(\"Age column converted to numeric.\")\n",
    "    print(\"Age column data type:\", all_data['Age'].dtype)\n",
    "    print(\"Age column info:\")\n",
    "    print(all_data['Age'].describe())\n",
    "else:\n",
    "    print(\"Age column not found. Skipping numeric conversion.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7138d50",
   "metadata": {},
   "source": [
    "### e. Bin Numerical Features ('Age' and 'Fare')\n",
    "\n",
    "We convert `Age` and `Fare` from continuous numbers to categorical bins. This helps models capture non-linear relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c5f3d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binned 'Age' into categories.\n",
      "Age categories distribution:\n",
      "Age_cat\n",
      "YoungAdult    806\n",
      "Child         248\n",
      "Adult         222\n",
      "Senior         33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Binned 'Fare' into categories.\n",
      "Fare categories distribution:\n",
      "Fare_cat\n",
      "Low         1242\n",
      "Medium        50\n",
      "Mid-High      13\n",
      "High           4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Bin Numerical Features ('Age' and 'Fare')\n",
    "# Use pd.cut instead of pd.qcut to avoid issues with duplicate values\n",
    "if 'Age' in all_data.columns:\n",
    "    all_data['Age_cat'] = pd.cut(all_data['Age'], bins=4, labels=['Child', 'YoungAdult', 'Adult', 'Senior'])\n",
    "    print(\"Binned 'Age' into categories.\")\n",
    "    print(\"Age categories distribution:\")\n",
    "    print(all_data['Age_cat'].value_counts())\n",
    "else:\n",
    "    print(\"Age column not found. Skipping Age binning.\")\n",
    "\n",
    "if 'Fare' in all_data.columns:\n",
    "    all_data['Fare_cat'] = pd.cut(all_data['Fare'], bins=4, labels=['Low', 'Medium', 'Mid-High', 'High'])\n",
    "    print(\"\\nBinned 'Fare' into categories.\")\n",
    "    print(\"Fare categories distribution:\")\n",
    "    print(all_data['Fare_cat'].value_counts())\n",
    "else:\n",
    "    print(\"Fare column not found. Skipping Fare binning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c8ce1",
   "metadata": {},
   "source": [
    "### f. Drop Unnecessary Columns & Encode Categorical Features\n",
    "\n",
    "Finally, we drop the original columns that are no longer needed and convert all remaining categorical features into numerical format using one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f2f007f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final processed data columns:\n",
      "Index(['PassengerId', 'Survived', 'Pclass_2', 'Pclass_3', 'Sex_male',\n",
      "       'Embarked_Q', 'Embarked_S', 'Deck_B', 'Deck_C', 'Deck_D', 'Deck_E',\n",
      "       'Deck_F', 'Deck_G', 'Deck_T', 'Deck_U', 'FamilySize_cat_Small',\n",
      "       'FamilySize_cat_Large', 'Age_cat_YoungAdult', 'Age_cat_Adult',\n",
      "       'Age_cat_Senior', 'Fare_cat_Medium', 'Fare_cat_Mid-High',\n",
      "       'Fare_cat_High'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Deck_B</th>\n",
       "      <th>Deck_C</th>\n",
       "      <th>Deck_D</th>\n",
       "      <th>...</th>\n",
       "      <th>Deck_T</th>\n",
       "      <th>Deck_U</th>\n",
       "      <th>FamilySize_cat_Small</th>\n",
       "      <th>FamilySize_cat_Large</th>\n",
       "      <th>Age_cat_YoungAdult</th>\n",
       "      <th>Age_cat_Adult</th>\n",
       "      <th>Age_cat_Senior</th>\n",
       "      <th>Fare_cat_Medium</th>\n",
       "      <th>Fare_cat_Mid-High</th>\n",
       "      <th>Fare_cat_High</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass_2  Pclass_3  Sex_male  Embarked_Q  \\\n",
       "0            1       0.0     False      True      True       False   \n",
       "1            2       1.0     False     False     False       False   \n",
       "2            3       1.0     False      True     False       False   \n",
       "3            4       1.0     False     False     False       False   \n",
       "4            5       0.0     False      True      True       False   \n",
       "\n",
       "   Embarked_S  Deck_B  Deck_C  Deck_D  ...  Deck_T  Deck_U  \\\n",
       "0        True   False   False   False  ...   False    True   \n",
       "1       False   False    True   False  ...   False   False   \n",
       "2        True   False   False   False  ...   False    True   \n",
       "3        True   False    True   False  ...   False   False   \n",
       "4        True   False   False   False  ...   False    True   \n",
       "\n",
       "   FamilySize_cat_Small  FamilySize_cat_Large  Age_cat_YoungAdult  \\\n",
       "0                  True                 False                True   \n",
       "1                  True                 False                True   \n",
       "2                 False                 False                True   \n",
       "3                  True                 False                True   \n",
       "4                 False                 False                True   \n",
       "\n",
       "   Age_cat_Adult  Age_cat_Senior  Fare_cat_Medium  Fare_cat_Mid-High  \\\n",
       "0          False           False            False              False   \n",
       "1          False           False            False              False   \n",
       "2          False           False            False              False   \n",
       "3          False           False            False              False   \n",
       "4          False           False            False              False   \n",
       "\n",
       "   Fare_cat_High  \n",
       "0          False  \n",
       "1          False  \n",
       "2          False  \n",
       "3          False  \n",
       "4          False  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop original columns\n",
    "all_data = all_data.drop(['Ticket', 'Cabin', 'Name', 'SibSp', 'Parch', 'Age', 'Fare', 'FamilySize'], axis=1)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "all_data = pd.get_dummies(all_data, columns=['Pclass', 'Sex', 'Embarked', 'Deck', 'FamilySize_cat', 'Age_cat', 'Fare_cat'], drop_first=True)\n",
    "\n",
    "print(\"Final processed data columns:\")\n",
    "print(all_data.columns)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3eb739",
   "metadata": {},
   "source": [
    "## Step 3: Model Training and Evaluation\n",
    "\n",
    "Now that our data is fully preprocessed, we can split it, train our models, and evaluate their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ccd72af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is split and scaled. Ready for training.\n"
     ]
    }
   ],
   "source": [
    "# Split data back into train and test sets\n",
    "train_processed = all_data[all_data['Survived'].notna()]\n",
    "test_processed = all_data[all_data['Survived'].isna()].drop('Survived', axis=1)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = train_processed.drop(\"Survived\", axis=1)\n",
    "y = train_processed[\"Survived\"].astype(int)\n",
    "\n",
    "# Create a validation set from the training data to evaluate models\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(\"Data is split and scaled. Ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dcb52e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models to be evaluated\n",
    "models = {\n",
    "    'Support Vector Machines': SVC(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Perceptron': Perceptron(),\n",
    "    'Stochastic Gradient Decent': SGDClassifier(),\n",
    "    'Linear SVC': LinearSVC(dual=False), # dual=False to avoid convergence warnings\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "model_accuracies = {}\n",
    "\n",
    "# Loop through each model, train it, and store its accuracy\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_val_scaled)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    model_accuracies[name] = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6033b",
   "metadata": {},
   "source": [
    "## Step 4: Display and Visualize Results\n",
    "\n",
    "Let's see which models performed the best with our improved features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a89a679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Accuracy Ranking ---\n",
      "                        Model  Accuracy\n",
      "0     Support Vector Machines  0.798883\n",
      "1         Logistic Regression  0.798883\n",
      "2                  Linear SVC  0.798883\n",
      "3                         KNN  0.787709\n",
      "4                  Perceptron  0.776536\n",
      "5  Stochastic Gradient Decent  0.776536\n",
      "6               Random Forest  0.748603\n",
      "7               Decision Tree  0.720670\n",
      "8                 Naive Bayes  0.709497\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to display accuracies\n",
    "accuracy_df = pd.DataFrame(list(model_accuracies.items()), columns=['Model', 'Accuracy'])\n",
    "accuracy_df = accuracy_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"--- Model Accuracy Ranking ---\")\n",
    "print(accuracy_df)\n",
    "\n",
    "# # Plotting the results for better visualization\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# sns.barplot(x='Accuracy', y='Model', data=accuracy_df, palette='viridis')\n",
    "# plt.title('Improved Model Classification Accuracies', fontsize=16)\n",
    "# plt.xlabel('Accuracy', fontsize=12)\n",
    "# plt.ylabel('Model', fontsize=12)\n",
    "# plt.xlim(0.7, 0.9) # Set x-axis limit for better readability\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff068874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708eec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
